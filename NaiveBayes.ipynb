{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPI0io59boNb8cAWEizt7Lv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwrno2eJE1sY","executionInfo":{"status":"ok","timestamp":1732450190354,"user_tz":-330,"elapsed":8278,"user":{"displayName":"Harshini V","userId":"05030960265327205852"}},"outputId":"c7fa1e49-849b-4886-8322-654110b563e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_c: <class 'numpy.ndarray'> (403, 10)\n","X_c: <class 'numpy.ndarray'> (397, 10)\n","Naive Bayes classification accuracy 0.965\n"]}],"source":["import numpy as np\n","\n","\n","class NaiveBayes:\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","        self._classes = np.unique(y)\n","        n_classes = len(self._classes)\n","\n","        # calculate mean, var, and prior for _each class_\n","        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n","        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n","        self._priors = np.zeros(n_classes, dtype=np.float64)\n","\n","        for idx, c in enumerate(self._classes):\n","            X_c = X[y == c]\n","            print('X_c:',type(X_c),X_c.shape)\n","            self._mean[idx, :] = X_c.mean(axis=0)\n","            self._var[idx, :] = X_c.var(axis=0)\n","            self._priors[idx] = X_c.shape[0] / float(n_samples)\n","\n","    def predict(self, X):\n","        y_pred = [self._predict(x) for x in X]\n","        return np.array(y_pred)\n","\n","    def _predict(self, x):\n","        posteriors = []\n","\n","        # calculate posterior probability for each class\n","        for idx, c in enumerate(self._classes):\n","            prior = np.log(self._priors[idx])\n","            posterior = np.sum(np.log(self._pdf(idx, x)))\n","            posterior = prior + posterior\n","            posteriors.append(posterior)\n","\n","        # return class with highest posterior probability\n","        return self._classes[np.argmax(posteriors)]\n","\n","    def _pdf(self, class_idx, x):\n","        mean = self._mean[class_idx]\n","        var = self._var[class_idx]\n","        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n","        denominator = np.sqrt(2 * np.pi * var)\n","        return numerator / denominator\n","\n","\n","# Testing\n","if __name__ == \"__main__\":\n","    # Imports\n","    from sklearn.model_selection import train_test_split\n","    from sklearn import datasets\n","\n","    def accuracy(y_true, y_pred):\n","        accuracy = np.sum(y_true == y_pred) / len(y_true)\n","        return accuracy\n","\n","    X, y = datasets.make_classification(\n","        n_samples=1000, n_features=10, n_classes=2, random_state=123\n","    )\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=123\n","    )\n","\n","    nb = NaiveBayes()\n","    nb.fit(X_train, y_train)\n","    predictions = nb.predict(X_test)\n","\n","    print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))"]},{"cell_type":"code","source":[],"metadata":{"id":"O13OGXjgE7Ph"},"execution_count":null,"outputs":[]}]}